# ğŸš¦ PerfReport MCP Server

Welcome to the **PerfReport MCP Server**!
This Python-based MCP server is built using FastMCP to generate easy-to-share, stakeholder-ready performance reports from your BlazeMeter and APM (e.g. Datadog, Dynatrace, AppDynamics, etc) analysis workflows.

---

## â­ Features

- ğŸ“ Generate beautiful performance test reports (Markdown, PDF, Word)
- ğŸ“Š Create PNG charts for single-axis, dual-axis, and multi-line visualizations
- ğŸ“ˆ Multi-line infrastructure charts showing all hosts/services on one chart
- ğŸ“‘ Template-driven formatting with chart placeholder support
- ğŸ—‚ Compare multiple runs in a single analysis with comparison bar charts
- ğŸ¤– AI-assisted report revision with Human-In-The-Loop (HITL) workflow
- ğŸ”„ Version-tracked revisions (v1, v2, v3...) for iterative refinement
- ğŸ”— Modular structure with seamless MCP suite integration
- ğŸ–¼ï¸ Confluence-ready chart filenames following schema ID conventions

---

## âš¡ Prerequisites

- Python 3.12.4 or higher
- Access to BlazeMeter and APM MCP artifacts
- Setup your `config.yaml` and `chart_colors.yaml` file

---

## ğŸš€ Getting Started

### 1. Clone the repository

```
git clone https://github.com/canyonlabz/mcp-perf-suite.git
cd perfreport-mcp
```

### 2. Create/activate virtual environment

A virtual environment can be manually activated, or automatically initialized by the MCP Client (e.g. Cursor) on startup.

#### On macOS / Linux
```
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

#### On Windows (PowerShell)
```
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

#### 3. Configure your environment

- Update `config.yaml` and `chart_colors.yaml`
- Ensure `templates/` contains required .md templates

#### 4. Running the MCP server â–¶ï¸

*Option 1: Run directly with Python*
`python perfreport.py`

*Option 2: Run using `uv` (Recommended) âš¡

You can use **uv** to simplify setup and execution. It manages dependencies and environments automatically.

- Install `uv` (macOS, Linux, Windows PowerShell)
```
curl -LsSf https://astral.sh/uv/install.sh | sh
```

- Run the MCP Server with `uv`

```
uv run perfreport.py
```

---

## ğŸ› MCP Tools

These are exposed for Cursor, agent, or CLI use:

### Report Generation Tools

| Tool | Description |
| :-- | :-- |
| `create_performance_test_report` | Generate a report (Markdown, PDF, Word) from a single test run |
| `create_comparison_report` | Compare multiple runs in one report |
| `list_templates` | Show available report templates |
| `get_template_details` | Show details/preview for a specific template |

### AI-Assisted Revision Tools

| Tool | Description |
| :-- | :-- |
| `discover_revision_data` | Scan artifacts to find all data files for AI revision |
| `prepare_revision_context` | Save AI-generated content for a report section (with version tracking) |
| `revise_performance_test_report` | Assemble final revised report from AI-generated sections |

### Chart Generation Tools

| Tool | Description |
| :-- | :-- |
| `create_chart` | Create a PNG chart by chart_id (single-axis, dual-axis, or multi-line) |
| `create_comparison_chart` | Create comparison bar charts for multiple test runs |
| `list_chart_types` | List all available chart types from chart_schema.yaml |

### ğŸ“Š Available Chart Types

#### Performance Charts

| Chart ID | Type | Description |
| :-- | :-- | :-- |
| `RESP_TIME_P90_VUSERS_DUALAXIS` | Dual-axis | P90 response time vs virtual users |

#### Infrastructure Charts (Utilization %)

| Chart ID | Type | Description |
| :-- | :-- | :-- |
| `CPU_UTILIZATION_LINE` | Single-axis | CPU % for a specific host/service |
| `CPU_UTILIZATION_VUSERS_DUALAXIS` | Dual-axis | CPU % vs virtual users |
| `CPU_UTILIZATION_MULTILINE` | Multi-line | CPU % for ALL hosts/services |
| `MEMORY_UTILIZATION_LINE` | Single-axis | Memory % for a specific host/service |
| `MEMORY_UTILIZATION_VUSERS_DUALAXIS` | Dual-axis | Memory % vs virtual users |
| `MEMORY_UTILIZATION_MULTILINE` | Multi-line | Memory % for ALL hosts/services |

#### Infrastructure Charts (Raw Usage)

| Chart ID | Type | Description |
| :-- | :-- | :-- |
| `CPU_CORES_LINE` | Single-axis | CPU core usage (millicores) for a host/service |
| `MEMORY_USAGE_LINE` | Single-axis | Memory usage (MB) for a host/service |

#### Comparison Charts (For Multi-Run Reports)

| Chart ID | Type | Description |
| :-- | :-- | :-- |
| `CPU_PEAK_CORE_COMPARISON_BAR` | Vertical bar | Compare peak CPU usage across test runs |
| `CPU_AVG_CORE_COMPARISON_BAR` | Vertical bar | Compare average CPU usage across test runs |
| `MEMORY_PEAK_USAGE_COMPARISON_BAR` | Vertical bar | Compare peak memory usage across test runs |
| `MEMORY_AVG_USAGE_COMPARISON_BAR` | Vertical bar | Compare average memory usage across test runs |

#### ğŸš§ Planned Charts (Not Yet Implemented)

| Chart ID | Type | Description |
| :-- | :-- | :-- |
| `ERROR_RATE_LINE` | Single-axis | Error occurrences over time |
| `THROUGHPUT_HITS_LINE` | Single-axis | Transaction throughput (req/sec) |
| `TOP_SLOWEST_APIS_BAR` | Horizontal bar | Top API SLA violators |
| `CPU_UTILIZATION_STACKED` | Stacked area | Cumulative CPU usage per service |
| `MEM_UTILIZATION_STACKED` | Stacked area | Cumulative memory usage per service |
| `CORR_HEATMAP_MATRIX` | Heatmap | Performance-infrastructure correlation matrix |

### ğŸ“ Chart Filename Conventions

Charts are saved to `artifacts/<run_id>/charts/` using standardized filenames:

| Chart Type | Filename Pattern | Example |
| :-- | :-- | :-- |
| Multi-line | `SCHEMA_ID.png` | `CPU_UTILIZATION_MULTILINE.png` |
| Performance | `SCHEMA_ID.png` | `RESP_TIME_P90_VUSERS_DUALAXIS.png` |
| Per-resource | `SCHEMA_ID-<resource>.png` | `CPU_UTILIZATION_LINE-api-gateway.png` |


---

## ğŸ”„ Workflow Example

1. ğŸƒâ€â™‚ï¸ Generate a report after test analysis
2. ğŸŒŸ Visualize results with charts for stakeholders
3. ğŸ‘¥ Revise reports with feedback (business, QA, engineering)
4. ğŸ“ˆ Compare test runs for trends and regression
5. ğŸ“‚ Download outputs from the artifacts directory

---

## ğŸ“ Output Examples

**Markdown Report**

```
# Performance Report: RUN-20251010-01
- SLA Met: All endpoints âœ…
- Peak throughput: 1200 req/sec
- Bottleneck: Database tier ğŸ”
```

**Returned JSON**

```json
{
  "run_id": "RUN-20251010-01",
  "path": "/artifacts/RUN-20251010-01/reports/performance_report.md"
}
```

**PNG Chart**

```
/artifacts/RUN-20251010-01/charts/response-time.png
```

---

## ğŸ— Project Structure

```
perfreport-mcp/
â”œâ”€â”€ perfreport.py                               # MCP entrypoint   
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ report_generator.py                     # Single-run report generation
â”‚   â”œâ”€â”€ comparison_report_generator.py          # Multi-run comparison reports
â”‚   â”œâ”€â”€ report_revision_generator.py            # AI-assisted report revision assembly
â”‚   â”œâ”€â”€ revision_data_discovery.py              # Discover data files for AI revision
â”‚   â”œâ”€â”€ revision_context_manager.py             # Save/manage AI revision content
â”‚   â”œâ”€â”€ chart_generator.py                      # Single-run chart generation
â”‚   â”œâ”€â”€ comparison_chart_generator.py           # Multi-run comparison charts
â”‚   â”œâ”€â”€ template_manager.py                     # Template reading/writing
â”‚   â””â”€â”€ charts/                                 # Chart type implementations
â”‚       â”œâ”€â”€ single_axis_charts.py               # Single-axis line charts
â”‚       â”œâ”€â”€ dual_axis_charts.py                 # Dual-axis line charts
â”‚       â”œâ”€â”€ multi_line_charts.py                # Multi-line overlay charts
â”‚       â””â”€â”€ comparison_bar_charts.py            # Vertical bar comparison charts
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config.py                               # Config loading utilities
â”‚   â”œâ”€â”€ data_loader_utils.py                    # Centralized data loading helper
â”‚   â”œâ”€â”€ revision_utils.py                       # Path helpers for revision workflow
â”‚   â”œâ”€â”€ chart_utils.py                          # Chart generation utilities
â”‚   â”œâ”€â”€ file_utils.py                           # File handling utilities
â”‚   â””â”€â”€ report_utils.py                         # Report generation utilities
â”œâ”€â”€ config.yaml                                 # Centralized, environment-agnostic config
â”œâ”€â”€ report_config.yaml                          # Report sections and revision settings
â”œâ”€â”€ chart_colors.yaml                           # Color palettes for charts
â”œâ”€â”€ chart_schema.yaml                           # Chart type definitions and specifications
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ default_report_template.md              # Default single-run report template
â”‚   â”œâ”€â”€ default_comparison_report_template.md   # Default comparison report template
â”‚   â””â”€â”€ ai_*.md                                 # AI-generated template variants
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml                              # Modern Python project metadata & dependencies
â””â”€â”€ requirements.txt                            # Dependencies
```


***

## ğŸ”Œ Integration

Works seamlessly with BlazeMeter, Datadog, PerfAnalysis, and Confluence MCP servers
for full-stack, end-to-end performance test reporting.

***

## ğŸ™Œ Contributing

ğŸ’¡ Suggestions, issues, and PRs are always welcome!
Built with FastMCP, Matplotlib, and love.