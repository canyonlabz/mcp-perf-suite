import sys
import json
import os
import urllib.parse
from typing import Callable, Optional, Dict, Any, List
from fastmcp import Context  # ‚úÖ FastMCP 2.x import

from utils.config import load_config, load_jmeter_config

# === Global configuration ===
CONFIG = load_config()
ARTIFACTS_PATH = CONFIG["artifacts"]["artifacts_path"]
JMETER_CONFIG = load_jmeter_config()

# === Domain Exclusion (APM, analytics, etc.) ===
from services.correlations.utils import init_exclude_domains, is_excluded_url
init_exclude_domains(CONFIG)

# Import necessary modules for JMeter JMX generation
import xml.etree.ElementTree as ET  # Needed for creating empty hashTree elements
from services.jmx.plan import (
    create_test_plan,
    create_thread_group
)
from services.jmx.controllers import (
    create_simple_controller,
    create_transaction_controller,
    # ‚Ä¶add more factory functions as you build them‚Ä¶
)
from services.jmx.samplers import (
    create_http_sampler_get,
    create_http_sampler_with_body,
    append_sampler
)
from services.jmx.config_elements import (
    create_cookie_manager,
    create_user_defined_variables,
    create_csv_data_set_config
)
from services.jmx.listeners import (
    create_view_results_tree,
    create_aggregate_report
)
from services.jmx.post_processor import (
    create_json_extractor,
    create_regex_extractor
)
from utils.file_utils import save_jmx_file

# ============================================================
# Helper Functions - Correlation Support
# ============================================================

def _load_correlation_naming(test_run_id: str) -> Optional[Dict[str, Any]]:
    """
    Load correlation_naming.json if it exists.
    
    This file is generated by applying Cursor Rules to correlation_spec.json
    and contains JMeter variable names and extractor configurations.
    
    Args:
        test_run_id: The test run identifier
        
    Returns:
        The correlation naming data, or None if file doesn't exist
    """
    naming_path = os.path.join(ARTIFACTS_PATH, test_run_id, "jmeter", "correlation_naming.json")
    if os.path.isfile(naming_path):
        with open(naming_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def _load_correlation_spec(test_run_id: str) -> Optional[Dict[str, Any]]:
    """
    Load correlation_spec.json if it exists.
    
    This file contains the detailed correlation analysis including actual values
    and usage locations needed for variable substitution.
    
    Args:
        test_run_id: The test run identifier
        
    Returns:
        The correlation spec data, or None if file doesn't exist
    """
    spec_path = os.path.join(ARTIFACTS_PATH, test_run_id, "jmeter", "correlation_spec.json")
    if os.path.isfile(spec_path):
        with open(spec_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None


def _normalize_url(url: str) -> str:
    """
    Normalize a URL for comparison by removing query parameters and fragments.
    
    Args:
        url: The URL to normalize
        
    Returns:
        The normalized URL (scheme://host/path)
    """
    parsed = urllib.parse.urlparse(url)
    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"


def _build_extractor_map(correlation_naming: Dict[str, Any]) -> Dict[str, List[Dict]]:
    """
    Build a mapping from source URL to list of extractor configurations.
    
    This allows quick lookup when creating HTTP samplers to determine
    which extractors should be added to each sampler's hashTree.
    
    Args:
        correlation_naming: The loaded correlation_naming.json data
        
    Returns:
        Dictionary mapping normalized URLs to lists of variable configs
    """
    extractor_map = {}
    
    for var in correlation_naming.get("variables", []):
        source_url = var.get("source_request_url", "")
        if not source_url:
            continue
            
        # Store both exact URL and normalized version for flexible matching
        normalized = _normalize_url(source_url)
        
        if normalized not in extractor_map:
            extractor_map[normalized] = []
        extractor_map[normalized].append(var)
        
        # Also store exact URL if different from normalized
        if source_url != normalized:
            if source_url not in extractor_map:
                extractor_map[source_url] = []
            # Avoid duplicates
            if var not in extractor_map[source_url]:
                extractor_map[source_url].append(var)
    
    return extractor_map


def _find_extractors_for_url(url: str, extractor_map: Dict[str, List[Dict]]) -> List[Dict]:
    """
    Find extractor configurations that match a given URL.
    
    Attempts multiple matching strategies:
    1. Exact URL match
    2. Normalized URL match (without query params)
    3. Partial match (URL contains or is contained by source URL path)
    
    Args:
        url: The URL of the current HTTP sampler
        extractor_map: The pre-built extractor mapping
        
    Returns:
        List of variable configurations that should extract from this URL
    """
    # Try exact match first
    if url in extractor_map:
        return extractor_map[url]
    
    # Try normalized URL match
    normalized = _normalize_url(url)
    if normalized in extractor_map:
        return extractor_map[normalized]
    
    # Try partial path matching for edge cases
    parsed = urllib.parse.urlparse(url)
    url_path = parsed.path.rstrip('/')
    
    for source_url, extractors in extractor_map.items():
        source_parsed = urllib.parse.urlparse(source_url)
        source_path = source_parsed.path.rstrip('/')
        
        # Match if same host and path matches
        if (parsed.netloc == source_parsed.netloc and 
            (url_path == source_path or 
             url_path.startswith(source_path + '/') or
             source_path.startswith(url_path + '/'))):
            return extractors
    
    return []


def _infer_field_to_check(expression: str, extractor_type: str) -> str:
    """
    Infer the 'field_to_check' parameter for regex extractors.
    
    Based on the expression pattern, determine where JMeter should
    search for the value (body, headers, or URL).
    
    Args:
        expression: The regex or JSONPath expression
        extractor_type: "json_extractor" or "regex_extractor"
        
    Returns:
        One of: "body", "headers", "url"
    """
    if extractor_type == "json_extractor":
        return "body"  # JSON is always in response body
    
    expression_lower = expression.lower()
    
    # URL query parameter pattern (e.g., [?&]code=([^&]+))
    if "[?&]" in expression or "\\?" in expression:
        return "url"
    
    # Header patterns (e.g., X-Header: (.+), Set-Cookie:, Location:)
    header_indicators = [
        "set-cookie", "location:", "x-", "authorization:",
        "content-type:", "www-authenticate:", "bearer"
    ]
    for indicator in header_indicators:
        if indicator in expression_lower:
            return "headers"
    
    # Cookie extraction patterns
    if "cookie" in expression_lower:
        return "headers"
    
    # Default to body
    return "body"


def _create_extractor_element(var_config: Dict) -> Optional[ET.Element]:
    """
    Create the appropriate JMeter extractor element for a correlation variable.
    
    Based on the jmeter_extractor_type, creates either a JSON Extractor
    or Regular Expression Extractor with the configured parameters.
    
    Args:
        var_config: Variable configuration from correlation_naming.json
        
    Returns:
        The extractor XML element, or None if invalid config
    """
    extractor_type = var_config.get("jmeter_extractor_type", "")
    variable_name = var_config.get("variable_name", "")
    expression = var_config.get("jmeter_extractor_expression", "")
    testname = var_config.get("jmeter_extractor_name", f"Extract {variable_name}")
    
    if not variable_name or not expression:
        return None
    
    if extractor_type == "json_extractor":
        return create_json_extractor(
            variable_name=variable_name,
            json_path=expression,
            testname=testname
        )
    elif extractor_type == "regex_extractor":
        field_to_check = _infer_field_to_check(expression, extractor_type)
        return create_regex_extractor(
            variable_name=variable_name,
            regex=expression,
            field_to_check=field_to_check,
            testname=testname
        )
    
    return None


def _get_extractors_for_entry(entry: Dict, extractor_map: Dict[str, List[Dict]]) -> List[ET.Element]:
    """
    Get all extractor elements that should be added to a sampler.
    
    Args:
        entry: The network capture entry (contains URL)
        extractor_map: The pre-built extractor mapping
        
    Returns:
        List of extractor XML elements to add to the sampler's hashTree
    """
    url = entry.get("url", "")
    if not url:
        return []
    
    var_configs = _find_extractors_for_url(url, extractor_map)
    extractors = []
    
    for var_config in var_configs:
        extractor = _create_extractor_element(var_config)
        if extractor is not None:
            extractors.append(extractor)
    
    return extractors


# ============================================================
# Helper Functions - Variable Substitution (Phase C)
# ============================================================

def _build_variable_name_map(correlation_naming: Dict[str, Any]) -> Dict[str, str]:
    """
    Build a mapping from correlation_id to variable_name.
    
    Args:
        correlation_naming: The loaded correlation_naming.json data
        
    Returns:
        Dictionary mapping correlation_id to variable_name
    """
    var_map = {}
    for var in correlation_naming.get("variables", []):
        corr_id = var.get("correlation_id", "")
        var_name = var.get("variable_name", "")
        if corr_id and var_name:
            var_map[corr_id] = var_name
    return var_map


def _build_substitution_map(
    correlation_spec: Dict[str, Any],
    variable_name_map: Dict[str, str]
) -> Dict[str, List[Dict]]:
    """
    Build a mapping from request URL to list of substitutions.
    
    For each usage in correlation_spec, creates a substitution entry
    that includes the value to replace and the variable name to use.
    
    Args:
        correlation_spec: The loaded correlation_spec.json data
        variable_name_map: Mapping from correlation_id to variable_name
        
    Returns:
        Dictionary mapping normalized request URLs to lists of substitutions.
        Each substitution contains:
        - value: The actual value to replace
        - variable_name: The JMeter variable name (e.g., "product_id")
        - location_type: Where to substitute (request_url_path, request_query_param, etc.)
        - location_key: The parameter/field name (e.g., "id", "engagementId")
    """
    sub_map = {}
    
    for corr in correlation_spec.get("correlations", []):
        corr_id = corr.get("correlation_id", "")
        variable_name = variable_name_map.get(corr_id)
        
        if not variable_name:
            continue  # Skip if no variable name defined
        
        # Get the actual value from source
        source = corr.get("source", {})
        value = source.get("response_example_value", "")
        
        if not value:
            continue  # Skip if no value to substitute
        
        # Process each usage
        for usage in corr.get("usages", []):
            request_url = usage.get("request_url", "")
            if not request_url:
                continue
            
            # Normalize the URL for matching
            normalized_url = _normalize_url(request_url)
            
            substitution = {
                "value": str(value),
                "variable_name": variable_name,
                "location_type": usage.get("location_type", ""),
                "location_key": usage.get("location_key", ""),
                "location_json_path": usage.get("location_json_path"),
                "request_id": usage.get("request_id", ""),
                "entry_index": usage.get("entry_index"),
            }
            
            # Store by both exact URL and normalized URL
            if normalized_url not in sub_map:
                sub_map[normalized_url] = []
            sub_map[normalized_url].append(substitution)
            
            # Also store exact URL if different
            if request_url != normalized_url:
                if request_url not in sub_map:
                    sub_map[request_url] = []
                # Avoid duplicates
                if substitution not in sub_map[request_url]:
                    sub_map[request_url].append(substitution)
    
    return sub_map


def _find_substitutions_for_url(url: str, sub_map: Dict[str, List[Dict]]) -> List[Dict]:
    """
    Find substitutions that should be applied to a given URL.
    
    Args:
        url: The URL of the current HTTP request entry
        sub_map: The pre-built substitution mapping
        
    Returns:
        List of substitution configurations for this URL
    """
    # Try exact match first
    if url in sub_map:
        return sub_map[url]
    
    # Try normalized URL match
    normalized = _normalize_url(url)
    if normalized in sub_map:
        return sub_map[normalized]
    
    return []


def _substitute_in_url(url: str, substitutions: List[Dict]) -> str:
    """
    Apply variable substitutions to a URL.
    
    Handles:
    - request_url_path: Replace value in URL path
    - request_query_param: Replace value in query parameters
    
    Args:
        url: The original URL
        substitutions: List of substitutions to apply
        
    Returns:
        The URL with hardcoded values replaced by ${variable_name}
    """
    result_url = url
    
    for sub in substitutions:
        value = sub.get("value", "")
        var_name = sub.get("variable_name", "")
        location_type = sub.get("location_type", "")
        
        if not value or not var_name:
            continue
        
        jmeter_var = f"${{{var_name}}}"
        
        if location_type in ("request_url_path", "request_query_param"):
            # URL-encoded version of the value
            encoded_value = urllib.parse.quote(value, safe='')
            
            # Replace both encoded and non-encoded versions
            result_url = result_url.replace(encoded_value, jmeter_var)
            result_url = result_url.replace(value, jmeter_var)
    
    return result_url


def _substitute_in_body(body: str, substitutions: List[Dict]) -> str:
    """
    Apply variable substitutions to a request body.
    
    Handles:
    - request_body_json: Replace value in JSON body
    - request_body_form: Replace value in form-encoded body
    
    Args:
        body: The original request body
        substitutions: List of substitutions to apply
        
    Returns:
        The body with hardcoded values replaced by ${variable_name}
    """
    if not body:
        return body
    
    result_body = body
    
    for sub in substitutions:
        value = sub.get("value", "")
        var_name = sub.get("variable_name", "")
        location_type = sub.get("location_type", "")
        
        if not value or not var_name:
            continue
        
        jmeter_var = f"${{{var_name}}}"
        
        if location_type in ("request_body_json", "request_body_form"):
            # For JSON bodies, handle both quoted and unquoted values
            # Numeric values in JSON are unquoted
            if value.isdigit():
                # Replace as number: "field": 123 -> "field": ${var}
                result_body = result_body.replace(f": {value}", f": {jmeter_var}")
                result_body = result_body.replace(f":{value}", f":{jmeter_var}")
            
            # Also replace as string: "field": "value" -> "field": "${var}"
            result_body = result_body.replace(f'"{value}"', f'"{jmeter_var}"')
            result_body = result_body.replace(f"'{value}'", f"'{jmeter_var}'")
            
            # Form-encoded: field=value -> field=${var}
            encoded_value = urllib.parse.quote(value, safe='')
            result_body = result_body.replace(f"={encoded_value}", f"={jmeter_var}")
            result_body = result_body.replace(f"={value}", f"={jmeter_var}")
    
    return result_body


def _substitute_in_headers(headers: Dict[str, str], substitutions: List[Dict]) -> Dict[str, str]:
    """
    Apply variable substitutions to request headers.
    
    Handles:
    - request_header: Replace value in header value
    
    Args:
        headers: The original headers dictionary
        substitutions: List of substitutions to apply
        
    Returns:
        The headers with hardcoded values replaced by ${variable_name}
    """
    if not headers:
        return headers
    
    result_headers = dict(headers)
    
    for sub in substitutions:
        value = sub.get("value", "")
        var_name = sub.get("variable_name", "")
        location_type = sub.get("location_type", "")
        location_key = sub.get("location_key", "")
        
        if not value or not var_name:
            continue
        
        if location_type != "request_header":
            continue
        
        jmeter_var = f"${{{var_name}}}"
        
        # If location_key specified, only substitute in that header
        if location_key and location_key in result_headers:
            result_headers[location_key] = result_headers[location_key].replace(value, jmeter_var)
        else:
            # Otherwise, substitute in all headers
            for key in result_headers:
                result_headers[key] = result_headers[key].replace(value, jmeter_var)
    
    return result_headers


def _apply_substitutions_to_entry(entry: Dict, sub_map: Dict[str, List[Dict]]) -> Dict:
    """
    Apply all variable substitutions to a network capture entry.
    
    Modifies the entry's URL, body, and headers to replace hardcoded
    correlation values with JMeter variable references.
    
    Args:
        entry: The network capture entry (will be modified in place)
        sub_map: The pre-built substitution mapping
        
    Returns:
        The modified entry (also modified in place)
    """
    url = entry.get("url", "")
    if not url:
        return entry
    
    # Find substitutions for this URL
    substitutions = _find_substitutions_for_url(url, sub_map)
    
    if not substitutions:
        return entry
    
    # Apply substitutions to URL
    entry["url"] = _substitute_in_url(url, substitutions)
    
    # Apply substitutions to body
    if "post_data" in entry and entry["post_data"]:
        entry["post_data"] = _substitute_in_body(entry["post_data"], substitutions)
    
    # Apply substitutions to headers
    if "headers" in entry and entry["headers"]:
        entry["headers"] = _substitute_in_headers(entry["headers"], substitutions)
    
    return entry


# ============================================================
# Helper Functions - Orphan Variable Handling (Phase D)
# ============================================================

def _extract_orphan_values(correlation_spec: Dict[str, Any]) -> Dict[str, str]:
    """
    Extract orphan ID values from correlation_spec.json.
    
    Orphan IDs are correlations where correlation_found is false.
    They have a value in source.response_example_value but no source response.
    
    Args:
        correlation_spec: The loaded correlation_spec.json data
        
    Returns:
        Dictionary mapping correlation_id to the actual value
    """
    orphan_values = {}
    
    for corr in correlation_spec.get("correlations", []):
        if not corr.get("correlation_found", True):
            corr_id = corr.get("correlation_id", "")
            source = corr.get("source", {})
            value = source.get("response_example_value", "")
            
            if corr_id and value:
                orphan_values[corr_id] = str(value)
    
    return orphan_values


def _get_orphan_udv_variables(
    correlation_naming: Dict[str, Any],
    orphan_values: Dict[str, str]
) -> Dict[str, str]:
    """
    Build User Defined Variables dictionary from orphan variables.
    
    Filters orphan variables where parameterization_strategy is "user_defined_variable"
    and joins with actual values from correlation_spec.
    
    Special handling:
    - SignalR timestamps (variable names containing 'signalr_timestamp' or 'signalr_ts'):
      Use JMeter's ${__time()} function instead of literal value
    
    Args:
        correlation_naming: The loaded correlation_naming.json data
        orphan_values: Mapping from correlation_id to actual value
        
    Returns:
        Dictionary of variable_name -> value suitable for UDV
    """
    udv_vars = {}
    
    for orphan in correlation_naming.get("orphan_variables", []):
        strategy = orphan.get("parameterization_strategy", "")
        
        # Only process user_defined_variable strategy
        if strategy != "user_defined_variable":
            continue
        
        corr_id = orphan.get("correlation_id", "")
        var_name = orphan.get("variable_name", "")
        
        if not var_name:
            continue
        
        # Get the actual value from orphan_values
        value = orphan_values.get(corr_id, "")
        
        # Special handling for SignalR timestamps
        # These should use ${__time()} function for dynamic generation
        if "signalr" in var_name.lower() and "timestamp" in var_name.lower():
            value = "${__time()}"
        elif "signalr" in var_name.lower() and "_ts" in var_name.lower():
            value = "${__time()}"
        
        if var_name and value:
            udv_vars[var_name] = value
    
    return udv_vars


def _merge_udv_config(
    base_config: Dict[str, Any],
    orphan_variables: Dict[str, str]
) -> Dict[str, Any]:
    """
    Merge orphan variables into the base UDV configuration.
    
    If there are orphan variables to add, ensures enabled=true and merges
    the variables dictionaries. Orphan variables do not override existing
    variables with the same name (config takes precedence).
    
    Args:
        base_config: The UDV configuration from jmeter_config.yaml
        orphan_variables: Dictionary of orphan variable_name -> value
        
    Returns:
        Merged configuration dictionary
    """
    if not orphan_variables:
        return base_config
    
    # Create a copy of base config
    merged = dict(base_config)
    
    # Enable UDV if we have orphan variables to add
    merged["enabled"] = True
    
    # Get existing variables (or empty dict)
    existing_vars = dict(merged.get("variables", {}))
    
    # Add orphan variables (don't override existing)
    for var_name, value in orphan_variables.items():
        if var_name not in existing_vars:
            existing_vars[var_name] = value
    
    merged["variables"] = existing_vars
    
    return merged


# ============================================================
# Helper Functions
# ============================================================



# ============================================================
# Main JMeter JMX Generator function
# ============================================================

async def generate_jmeter_jmx(test_run_id: str, json_path: str, ctx: Context) -> Dict[str, Any]:
    """
    Generate a JMeter JMX script from a network capture JSON file.

    This version is MCP-friendly and matches the JMeter MCP tool signature:

      - test_run_id: used to resolve output path: artifacts/<test_run_id>/jmeter/
      - json_path: full path to the network capture JSON
      - ctx: FastMCP Context object for logging (ctx.info / ctx.error, etc.)

    Returns:
      {
        "status": "success" | "error",
        "jmx_path": "<full path to .jmx file (if success)>",
        "message": "<human readable status>",
      }
    """
    # === Network Capture File ===
    # Check if the provided JSON file exists and is valid.
    if not json_path or not os.path.isfile(json_path):
        # If the file path is empty or does not exist, print an error message and exit.
        ctx.error(f"Error: No JSON file provided or file does not exist for given: '{json_path}'")
        raise ValueError(f"No JSON file provided or file does not exist for given: '{json_path}'")
    # Check if the file is a valid JSON file.
    if not json_path.endswith('.json'):
        # If the file is not a JSON file, print an error message and exit.
        ctx.error(f"Error: File '{json_path}' is not a valid JSON file.")
        raise ValueError(f"File '{json_path}' is not a valid JSON file.")
    
    # Load the network capture JSON file.
    with open(json_path, "r", encoding="utf-8") as f:
        network_data = json.load(f)

    ctx.info(f"‚úÖ Loaded network capture JSON file: {json_path}")
    ctx.info(f"Network data contains {len(network_data)} entries.")
    
    # === Load Correlation Data (if available) ===
    # correlation_naming.json: JMeter variable names and extractor configurations
    # correlation_spec.json: Actual values and usage locations for substitution
    correlation_naming = _load_correlation_naming(test_run_id)
    correlation_spec = _load_correlation_spec(test_run_id)
    
    extractor_map = {}
    substitution_map = {}
    orphan_udv_vars = {}  # Orphan variables for User Defined Variables (Phase D)
    
    if correlation_naming:
        extractor_map = _build_extractor_map(correlation_naming)
        var_count = len(correlation_naming.get("variables", []))
        orphan_count = len(correlation_naming.get("orphan_variables", []))
        ctx.info(f"‚úÖ Loaded correlation naming: {var_count} variables, {orphan_count} orphans")
        
        # Build substitution map and extract orphan values if we have correlation_spec
        if correlation_spec:
            variable_name_map = _build_variable_name_map(correlation_naming)
            substitution_map = _build_substitution_map(correlation_spec, variable_name_map)
            total_subs = sum(len(subs) for subs in substitution_map.values())
            ctx.info(f"‚úÖ Built substitution map: {total_subs} substitutions across {len(substitution_map)} URLs")
            
            # Extract orphan values and build UDV variables (Phase D)
            orphan_values = _extract_orphan_values(correlation_spec)
            orphan_udv_vars = _get_orphan_udv_variables(correlation_naming, orphan_values)
            if orphan_udv_vars:
                ctx.info(f"‚úÖ Extracted {len(orphan_udv_vars)} orphan variable(s) for User Defined Variables")
        else:
            ctx.info("‚ÑπÔ∏è No correlation_spec.json found - skipping variable substitution")
    else:
        ctx.info("‚ÑπÔ∏è No correlation_naming.json found - generating JMX without extractors or substitutions")
    
    # ============================================================
    # === JMeter JMX File Configurations ===
    # ============================================================

    # === Create JMeter Test Plan ===
    # Create the root Test Plan and its hashTree.
    test_plan, test_plan_hash_tree = create_test_plan()

    # === Add Optional Test Plan Elements ===
    # Add Cookie Manager if enabled.
    cookie_mgr_cfg = JMETER_CONFIG.get("cookie_manager", {"enabled": False})
    if cookie_mgr_cfg.get("enabled", False):
        cookie_manager_elem = create_cookie_manager()
        test_plan_hash_tree.append(cookie_manager_elem)
        test_plan_hash_tree.append(ET.Element("hashTree"))
    
    # Add User Defined Variables (merge with orphan variables from Phase D)
    udv_cfg = JMETER_CONFIG.get("user_defined_variables", {"enabled": False})
    
    # Merge orphan variables into UDV config if any were extracted
    if orphan_udv_vars:
        udv_cfg = _merge_udv_config(udv_cfg, orphan_udv_vars)
    
    udv_elem = create_user_defined_variables(udv_cfg)
    if udv_elem is not None:
        test_plan_hash_tree.append(udv_elem)
        test_plan_hash_tree.append(ET.Element("hashTree"))
    
    # Add CSV Data Set Config if enabled.
    csv_cfg = JMETER_CONFIG.get("csv_dataset_config", {"enabled": False})
    csv_elem = create_csv_data_set_config(csv_cfg)
    if csv_elem is not None:
        test_plan_hash_tree.append(csv_elem)
        test_plan_hash_tree.append(ET.Element("hashTree"))

    # === Create Thread Group ===
    # Create a single Thread Group using defaults from jmeter_config.
    tg_config = JMETER_CONFIG.get("thread_group", {})
    num_threads = str(tg_config.get("num_threads", 1))
    ramp_time = str(tg_config.get("ramp_time", 1))
    loops = str(tg_config.get("loops", 1))
    thread_group, thread_group_hash_tree = create_thread_group(
        num_threads=num_threads, ramp_time=ramp_time, loops=loops
    )
    # Append the thread group and its hashTree to the Test Plan.
    test_plan_hash_tree.append(thread_group)
    test_plan_hash_tree.append(thread_group_hash_tree)
    
    # === Create HTTP Request Samplers (possibly grouped in Controllers) ===
    ctrl_cfg = JMETER_CONFIG.get("controller_config", {})
    use_controllers = ctrl_cfg.get("enabled", False)

    # Track correlation additions for logging
    extractors_added = 0
    substitutions_applied = 0
    excluded_entries = 0
    
    if use_controllers:
        ctrl_type = ctrl_cfg.get("controller_type", "simple").lower()
        # pick factory based on type
        factory = {
            "simple": create_simple_controller,
            "transaction": create_transaction_controller,
            # Add more controller types as needed
        }.get(ctrl_type, create_simple_controller)

        for step_name, entries in network_data.items():
            # create the Controller node + its hashTree
            ctrl_elem, ctrl_hash = factory(testname=step_name)
            thread_group_hash_tree.append(ctrl_elem)
            thread_group_hash_tree.append(ctrl_hash)

            # Now append each sampler under this controller
            for entry in entries:
                # Filter out excluded domains (APM, analytics, advertising, etc.)
                entry_url = entry.get("url", "")
                if is_excluded_url(entry_url):
                    excluded_entries += 1
                    continue
                # Apply variable substitutions before creating sampler (Phase C)
                original_url = entry.get("url", "")
                if substitution_map:
                    _apply_substitutions_to_entry(entry, substitution_map)
                    # Count if URL changed (substitution was applied)
                    if entry.get("url", "") != original_url:
                        substitutions_applied += 1
                
                method = entry.get("method", "GET").upper()
                if method == "GET":
                    sampler, header_manager = create_http_sampler_get(entry)
                else:
                    sampler, header_manager = create_http_sampler_with_body(entry)
                
                # Get extractors for this URL (correlation support - Phase B)
                # Note: Use original URL for extractor lookup since that matches correlation_naming
                entry_for_extractor = {"url": original_url} if original_url else entry
                extractors = _get_extractors_for_entry(entry_for_extractor, extractor_map)
                extractors_added += len(extractors)
                
                append_sampler(ctrl_hash, sampler, header_manager, extractors=extractors)
    else:
        # === Create HTTP Request Samplers ===
        # Iterate through the network capture entries.
        # Assume network_data is a dictionary where keys are URLs and values are entry dicts.
        for url, entry in network_data.items():
            # Ensure each entry has its "url" field.
            if "url" not in entry:
                entry["url"] = url
            
            # Filter out excluded domains (APM, analytics, advertising, etc.)
            entry_url = entry.get("url", "")
            if is_excluded_url(entry_url):
                excluded_entries += 1
                continue
            
            # Apply variable substitutions before creating sampler (Phase C)
            original_url = entry.get("url", "")
            if substitution_map:
                _apply_substitutions_to_entry(entry, substitution_map)
                # Count if URL changed (substitution was applied)
                if entry.get("url", "") != original_url:
                    substitutions_applied += 1
            
            method = entry.get("method", "GET").upper()
            if method == "GET":
                sampler, header_manager = create_http_sampler_get(entry)
            else:
                sampler, header_manager = create_http_sampler_with_body(entry)
            
            # Get extractors for this URL (correlation support - Phase B)
            # Note: Use original URL for extractor lookup since that matches correlation_naming
            entry_for_extractor = {"url": original_url} if original_url else entry
            extractors = _get_extractors_for_entry(entry_for_extractor, extractor_map)
            extractors_added += len(extractors)
            
            # Append the sampler and its header manager (if any) into the Thread Group's hashTree.
            append_sampler(thread_group_hash_tree, sampler, header_manager, extractors=extractors)
    
    # Log correlation summary
    if excluded_entries > 0:
        ctx.info(f"üö´ Excluded {excluded_entries} request(s) from non-essential domains (APM, analytics, etc.)")
    if extractors_added > 0:
        ctx.info(f"‚úÖ Added {extractors_added} extractor(s) for correlation support")
    if substitutions_applied > 0:
        ctx.info(f"‚úÖ Applied variable substitutions to {substitutions_applied} request(s)")

    # === Add Listeners (outside the Thread Group) ===
    results_cfg = JMETER_CONFIG.get("results_collector_config", {})
    
    # Add View Results Tree if enabled.
    if results_cfg.get("view_results_tree", True):
        view_results_tree_settings = results_cfg.get("view_results_tree_settings", {})
        vrt_elem, vrt_hash_tree = create_view_results_tree(view_results_tree_settings)
        test_plan_hash_tree.append(vrt_elem)
        test_plan_hash_tree.append(vrt_hash_tree)

    # Add Aggregate Report if enabled.
    if results_cfg.get("aggregate_report", True):
        aggregate_report_settings = results_cfg.get("aggregate_report_settings", {})
        ar_elem, ar_hash_tree = create_aggregate_report(aggregate_report_settings)
        test_plan_hash_tree.append(ar_elem)
        test_plan_hash_tree.append(ar_hash_tree)

    # (You can add additional listeners here, e.g., Aggregate Report, Response Time Graph, etc.)

    try:
   	    # Save the complete JMX file to the output directory.
        # Write to artifacts/<test_run_id>/jmeter/.
        jmx_path = save_jmx_file(test_plan, test_run_id)

        if not jmx_path:
            msg = "‚ùå Failed to generate JMX file."
            ctx.error(msg)
            return {
	        	"status": "error",
	        	"jmx_path": "",
	        	"message": msg
	        }

        msg = f"JMX script generated successfully: {jmx_path}"
        ctx.info(msg)
        return {
            "status": "success",
            "jmx_path": jmx_path,
            "message": msg
        }

    except Exception as e:
        msg = f"Failed to save JMX script: {e}"
        ctx.error(msg)
        return {
            "status": "error",
            "jmx_path": "",
            "message": msg,
        }
